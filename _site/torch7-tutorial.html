<!DOCTYPE html>
<html âš¡ lang="en">
  <head>
  <script async custom-element="amp-youtube" src="https://cdn.ampproject.org/v0/amp-youtube-0.1.js"></script>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width,minimum-scale=1">

  <title>Torch Tutorial for PlantVillage Challenge</title>
  <meta name="description" content="There is this interesting challenge called PlantVillage challenge hosted on a newly built platform, crowdai.In this challenge, you are required to identify t...">

  <link rel="canonical" href="https://chsasank.github.io//torch7-tutorial.html">
  <link rel="alternate" type="application/rss+xml" title="Sasank's Blog" href="https://chsasank.github.io//feed.xml">

  <script type="application/ld+json">
  
{
  "@context": "http://schema.org",
  "@type": "NewsArticle",
  "mainEntityOfPage": "https://chsasank.github.io//torch7-tutorial.html",
  "headline": "Torch Tutorial for PlantVillage Challenge",
  "datePublished": "2016-05-21T00:00:00+05:30",
  "dateModified": "2016-05-21T00:00:00+05:30",
  "description": "There is this interesting challenge called PlantVillage challenge hosted on a newly built platform, crowdai.In this challenge, you are required to identify t...",
  "author": {
    "@type": "Person",
    "name": "Sasank Chilamkurthy"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Sasank's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://chsasank.github.io/",
      "width": 60,
      "height": 60
    }
  },
  "image": {
    "@type": "ImageObject",
    "url": "https://chsasank.github.io/",
    "height": 60,
    "width": 60
  }
}

  </script>

  <style amp-custom>
  
  /* Import ET Book styles
   adapted from https://github.com/edwardtufte/et-book/blob/gh-pages/et-book.css */
@font-face {
  font-family: "et-book";
  src: url("/assets/et-book/et-book-roman-line-figures/et-book-roman-line-figures.eot");
  src: url("/assets/et-book/et-book-roman-line-figures/et-book-roman-line-figures.eot?#iefix") format("embedded-opentype"), url("/assets/et-book/et-book-roman-line-figures/et-book-roman-line-figures.woff") format("woff"), url("/assets/et-book/et-book-roman-line-figures/et-book-roman-line-figures.ttf") format("truetype"), url("/assets/et-book/et-book-roman-line-figures/et-book-roman-line-figures.svg#etbookromanosf") format("svg");
  font-weight: normal;
  font-style: normal; }
@font-face {
  font-family: "et-book";
  src: url("/assets/et-book/et-book-display-italic-old-style-figures/et-book-display-italic-old-style-figures.eot");
  src: url("/assets/et-book/et-book-display-italic-old-style-figures/et-book-display-italic-old-style-figures.eot?#iefix") format("embedded-opentype"), url("/assets/et-book/et-book-display-italic-old-style-figures/et-book-display-italic-old-style-figures.woff") format("woff"), url("/assets/et-book/et-book-display-italic-old-style-figures/et-book-display-italic-old-style-figures.ttf") format("truetype"), url("/assets/et-book/et-book-display-italic-old-style-figures/et-book-display-italic-old-style-figures.svg#etbookromanosf") format("svg");
  font-weight: normal;
  font-style: italic; }
@font-face {
  font-family: "et-book";
  src: url("/assets/et-book/et-book-bold-line-figures/et-book-bold-line-figures.eot");
  src: url("/assets/et-book/et-book-bold-line-figures/et-book-bold-line-figures.eot?#iefix") format("embedded-opentype"), url("/assets/et-book/et-book-bold-line-figures/et-book-bold-line-figures.woff") format("woff"), url("/assets/et-book/et-book-bold-line-figures/et-book-bold-line-figures.ttf") format("truetype"), url("/assets/et-book/et-book-bold-line-figures/et-book-bold-line-figures.svg#etbookromanosf") format("svg");
  font-weight: bold;
  font-style: normal; }
@font-face {
  font-family: "et-book-roman-old-style";
  src: url("/assets/et-book/et-book-roman-old-style-figures/et-book-roman-old-style-figures.eot");
  src: url("/assets/et-book/et-book-roman-old-style-figures/et-book-roman-old-style-figures.eot?#iefix") format("embedded-opentype"), url("/assets/et-book/et-book-roman-old-style-figures/et-book-roman-old-style-figures.woff") format("woff"), url("/assets/et-book/et-book-roman-old-style-figures/et-book-roman-old-style-figures.ttf") format("truetype"), url("/assets/et-book/et-book-roman-old-style-figures/et-book-roman-old-style-figures.svg#etbookromanosf") format("svg");
  font-weight: normal;
  font-style: normal; }
/* Tufte CSS styles */
html {
  font-size: 15px; }

body {
  width: 87.5%;
  margin-left: auto;
  margin-right: auto;
  padding-left: 12.5%;
  font-family: et-book, Palatino, "Palatino Linotype", "Palatino LT STD", "Book Antiqua", Georgia, serif;
  background-color: #fffff8;
  color: #111;
  max-width: 1400px;
  counter-reset: sidenote-counter; }

h1 {
  font-weight: 400;
  margin-top: 4rem;
  margin-bottom: 1.5rem;
  font-size: 3.2rem;
  line-height: 1; }

h2 {
  font-style: italic;
  font-weight: 400;
  margin-top: 2.1rem;
  margin-bottom: 0;
  font-size: 2.2rem;
  line-height: 1; }

h3 {
  font-style: italic;
  font-weight: 400;
  font-size: 1.7rem;
  margin-top: 2rem;
  margin-bottom: 0;
  line-height: 1; }

p.subtitle {
  font-style: italic;
  margin-top: 1rem;
  margin-bottom: 1rem;
  font-size: 1.8rem;
  display: block;
  line-height: 1; }

.numeral {
  font-family: et-book-roman-old-style; }

.danger {
  color: red; }

article {
  position: relative;
  padding: 5rem 0rem; }

section {
  padding-top: 1rem;
  padding-bottom: 1rem; }

p, ol, ul, .pagination a, .pagination em {
  font-size: 1.4rem; }

p {
  line-height: 2rem;
  margin-top: 1.4rem;
  margin-bottom: 1.4rem;
  padding-right: 0;
  vertical-align: baseline; }

/* Chapter Epigraphs */
div.epigraph {
  margin: 5em 0; }

div.epigraph > blockquote {
  margin-top: 3em;
  margin-bottom: 3em; }

div.epigraph > blockquote, div.epigraph > blockquote > p {
  font-style: italic; }

div.epigraph > blockquote > footer {
  font-style: normal; }

div.epigraph > blockquote > footer > cite {
  font-style: italic; }

/* end chapter epigraphs styles */
blockquote {
  font-size: 1.4rem; }

blockquote p {
  width: 50%; }

blockquote .footer {
  width: 50%;
  font-size: 1.1rem;
  text-align: right; }

ol, ul {
  width: 45%;
  -webkit-padding-start: 5%;
  -webkit-padding-end: 5%; }

li {
  padding: 0.5rem 0; }

figure {
  padding: 0;
  border: 0;
  font-size: 100%;
  font: inherit;
  vertical-align: baseline;
  max-width: 55%;
  -webkit-margin-start: 0;
  -webkit-margin-end: 0;
  margin: 0 0 3em 0; }

figcaption {
  float: right;
  clear: right;
  margin-right: -48%;
  margin-top: 0;
  margin-bottom: 0;
  font-size: 1.1rem;
  line-height: 1.6;
  vertical-align: baseline;
  position: relative;
  max-width: 40%; }

figure.fullwidth figcaption {
  margin-right: 24%; }

/* Links: replicate underline that clears descenders */
a:link, a:visited {
  color: inherit; }

a:link {
  text-decoration: none;
  background: -webkit-linear-gradient(#fffff8, #fffff8), -webkit-linear-gradient(#fffff8, #fffff8), -webkit-linear-gradient(#333, #333);
  background: linear-gradient(#fffff8, #fffff8), linear-gradient(#fffff8, #fffff8), linear-gradient(#333, #333);
  -webkit-background-size: 0.05em 1px, 0.05em 1px, 1px 1px;
  -moz-background-size: 0.05em 1px, 0.05em 1px, 1px 1px;
  background-size: 0.05em 1px, 0.05em 1px, 1px 1px;
  background-repeat: no-repeat, no-repeat, repeat-x;
  text-shadow: 0.03em 0 #fffff8, -0.03em 0 #fffff8, 0 0.03em #fffff8, 0 -0.03em #fffff8, 0.06em 0 #fffff8, -0.06em 0 #fffff8, 0.09em 0 #fffff8, -0.09em 0 #fffff8, 0.12em 0 #fffff8, -0.12em 0 #fffff8, 0.15em 0 #fffff8, -0.15em 0 #fffff8;
  background-position: 0% 93%, 100% 93%, 0% 93%; }

@media screen and (-webkit-min-device-pixel-ratio: 0) {
  a:link {
    background-position-y: 87%, 87%, 87%; } }
a:link::selection {
  text-shadow: 0.03em 0 #b4d5fe, -0.03em 0 #b4d5fe, 0 0.03em #b4d5fe, 0 -0.03em #b4d5fe, 0.06em 0 #b4d5fe, -0.06em 0 #b4d5fe, 0.09em 0 #b4d5fe, -0.09em 0 #b4d5fe, 0.12em 0 #b4d5fe, -0.12em 0 #b4d5fe, 0.15em 0 #b4d5fe, -0.15em 0 #b4d5fe;
  background: #b4d5fe; }

a:link::-moz-selection {
  text-shadow: 0.03em 0 #b4d5fe, -0.03em 0 #b4d5fe, 0 0.03em #b4d5fe, 0 -0.03em #b4d5fe, 0.06em 0 #b4d5fe, -0.06em 0 #b4d5fe, 0.09em 0 #b4d5fe, -0.09em 0 #b4d5fe, 0.12em 0 #b4d5fe, -0.12em 0 #b4d5fe, 0.15em 0 #b4d5fe, -0.15em 0 #b4d5fe;
  background: #b4d5fe; }

/* Sidenotes, margin notes, figures, captions */
img {
  max-width: 100%; }

.sidenote, .marginnote {
  float: right;
  clear: right;
  margin-right: -60%;
  width: 50%;
  margin-top: 0;
  margin-bottom: 0;
  font-size: 1.1rem;
  line-height: 1.3;
  vertical-align: baseline;
  position: relative; }

.table-caption {
  float: right;
  clear: right;
  margin-right: -60%;
  width: 50%;
  margin-top: 0;
  margin-bottom: 0;
  font-size: 1.0rem;
  line-height: 1.6; }

.sidenote-number {
  counter-increment: sidenote-counter; }

.sidenote-number:after, .sidenote:before {
  content: counter(sidenote-counter) " ";
  font-family: et-book-roman-old-style;
  position: relative;
  vertical-align: baseline; }

.sidenote-number:after {
  content: counter(sidenote-counter);
  font-size: 1rem;
  top: -0.5rem;
  left: 0.1rem; }

.sidenote:before {
  content: counter(sidenote-counter) " ";
  top: -0.5rem; }

p, footer, table, div.table-wrapper-small, div.supertable-wrapper > p, div.booktabs-wrapper {
  width: 55%; }

div.fullwidth, table.fullwidth {
  width: 100%; }

div.table-wrapper {
  overflow-x: auto;
  font-family: "Trebuchet MS", "Gill Sans", "Gill Sans MT", sans-serif; }

@media screen and (max-width: 760px) {
  p, h1, h2, h3, footer {
    width: 90%; }

  pre.code {
    width: 87.5%; }

  ul {
    width: 85%; }

  figure {
    max-width: 90%; }

  figcaption, figure.fullwidth figcaption {
    margin-right: 0%;
    max-width: none; }

  blockquote p, blockquote .footer {
    width: 90%; } }
.sans {
  font-family: "Gill Sans", "Gill Sans MT", Calibri, sans-serif;
  letter-spacing: .03em; }

.code {
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  font-size: 1.125rem;
  line-height: 1.6; }

h1 .code, h2 .code, h3 .code {
  font-size: 0.80em; }

.marginnote .code, .sidenote .code {
  font-size: 1rem; }

pre.code {
  width: 52.5%;
  padding-left: 2.5%;
  overflow-x: auto; }

.fullwidth {
  max-width: 90%;
  clear: both; }

span.newthought {
  font-variant: small-caps;
  font-size: 1.2em; }

.margin-toggle {
  display: none; }

.sidenote-number {
  display: inline; }

.margin-toggle:not(.sidenote-number) {
  display: none; }

@media (max-width: 760px) {
  .margin-toggle:not(.sidenote-number) {
    display: none; }

  .sidenote, .marginnote {
    display: none; }

  .margin-toggle:checked + .sidenote,
  .margin-toggle:checked + .marginnote {
    display: block;
    float: left;
    left: 1rem;
    clear: both;
    width: 95%;
    margin: 1rem 2.5%;
    vertical-align: baseline;
    position: relative; }

  label {
    cursor: pointer; }

  pre.code {
    width: 90%;
    padding: 0; }

  .table-caption {
    display: block;
    float: right;
    clear: both;
    width: 98%;
    margin-top: 1rem;
    margin-bottom: 0.5rem;
    margin-left: 1%;
    margin-right: 1%;
    vertical-align: baseline;
    position: relative; }

  div.table-wrapper, table, table.booktabs {
    width: 85%; }

  div.table-wrapper {
    border-right: 1px solid #efefef; }

  img {
    width: 100%; } }
main {
  margin-top: 20px; }

amp-img {
  background-color: grey; }

article {
  padding: 2.5rem 0; }

header {
  margin-top: 20px; }

.post-meta {
  margin-top: 10px; }

pre {
  width: 52.5%;
  padding-left: 2.5%;
  overflow-x: auto; }

@media (max-width: 760px) {
  pre {
    width: 90%;
    padding: 0; } }
code {
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  font-size: 1.125rem;
  line-height: 1.6; }

  </style>



  <style amp-boilerplate>body{-webkit-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-moz-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-ms-animation:-amp-start 8s steps(1,end) 0s 1 normal both;animation:-amp-start 8s steps(1,end) 0s 1 normal both}@-webkit-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-moz-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-ms-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-o-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}</style><noscript><style amp-boilerplate>body{-webkit-animation:none;-moz-animation:none;-ms-animation:none;animation:none}</style></noscript>

  <script async src="https://cdn.ampproject.org/v0.js"></script>
</head>

  <body>
    <header>
  <div class="page-links">
    <a class="page-link" href="/">Home</a>
    
      
      â€¢ <a class="page-link" href="/about/">About</a>
      
    
      
      â€¢ <a class="page-link" href="/contact/">Contact</a>
      
    
      
    
      
    
  </div>
</header>


    <article>

  <h2 itemprop="name"><a href="" itemprop="url">Torch Tutorial for PlantVillage Challenge</a></h2>

  <div class="post-meta">
    <time datetime="21 May 2016">21 May 2016</time>
  </div>

  <section>
    <p>There is this interesting challenge called <a href="https://www.crowdai.org/challenges/1">PlantVillage challenge</a> hosted on a newly built platform, <a href="https://www.crowdai.org">crowdai</a>.
In this challenge, you are required to identify the disease of a plant from an image of its leaf.</p>

<p>Dataset include both healthy and diseased leaves. Training dataset has 21917 images.
There are 38 classes of crop-disease pairs in the dataset:</p>

<figure>
  <amp-img width="600" height="500" layout="responsive" src="/assets/images/plantvillage/classes.jpg"></amp-img>
</figure>

<p>Weâ€™ll use popular deep learning platform <a href="http://torch.ch">torch</a> to solve this problem.
This will be a hands-on tutorial covering training of 
Alexnet
<span id="alexnet" class="margin-toggle sidenote-number"></span>
      <span class="sidenote"><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">ImageNet Classification with Deep Convolutional Neural Networks</a></span>.
Tutorial will be accompanied by a <a href="https://github.com/chsasank/plantvillage-challenge">repo containing complete working code</a>.
It will include VGGNet 
<span id="vggnet" class="margin-toggle sidenote-number"></span>
      <span class="sidenote"><a href="http://arxiv.org/abs/1409.1556">Very Deep Convolutional Networks for Large-Scale Image Recognition</a></span>
and ResNet 
<span id="resnet" class="margin-toggle sidenote-number"></span>
      <span class="sidenote"><a href="https://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a></span>.</p>

<p>This tutorial assumes familiarity with convolutional neural networks. If you are not, you can go through very readable <em>Neural Networks and Deep Learning</em> book by Michael Nielsen.
<a href="http://neuralnetworksanddeeplearning.com/chap6.html">Chapter 6</a> is the essential reading. 
Just read the <a href="http://neuralnetworksanddeeplearning.com/chap6.html#introducing_convolutional_networks">first section</a> in this chapter if you are in a hurry. Go on, itâ€™s a easy read. Iâ€™ll wait for you :).</p>

<p>Ok, Done? Letâ€™s first start with preprocessing and augmentation of the images.</p>

<h2 id="preprocessing">Preprocessing</h2>
<p>Firstly, letâ€™s download the dataset and extract it into a directory. 
Weâ€™ll divide the images into two directories, <code class="highlighter-rouge">train</code> and <code class="highlighter-rouge">val</code> for training and validation sets respectively.
I used a simple bash script to do this:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="nb">cd </span>directory/contaning/c_0c_1...etcdirectories
mkdir -p train val
<span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>0..37<span class="o">}</span>; <span class="k">do </span>mkdir val/c_<span class="nv">$i</span>; <span class="k">done
</span>mv c_<span class="k">*</span> train

<span class="nb">cd </span>train
find . -iname <span class="k">*</span>.jpg | shuf | head -n 2100| xargs -I<span class="o">{}</span> mv <span class="o">{}</span> ../val/<span class="o">{}</span>
</code></pre>
</div>

<p>This will move random 2100 images (about 10% of the dataset) in to <code class="highlighter-rouge">val</code> directory and rest into <code class="highlighter-rouge">train</code> directory.
Directory structure should now look like:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>.
â”œâ”€â”€ train
â”‚Â Â  â”œâ”€â”€ c_0
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ img_name.JPG
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ...
â”‚Â Â  â”‚Â Â  â””â”€â”€ img_name.JPG
â”‚Â Â  â”œâ”€â”€ c_1
â”‚Â Â  â”œâ”€â”€ ...
â”‚Â Â  â”œâ”€â”€ c_36
â”‚Â Â  â””â”€â”€ c_37
â”‚Â Â 
â””â”€â”€ val
    â”œâ”€â”€ c_0
    â”œâ”€â”€ c_1
    â”œâ”€â”€ ...
    â”œâ”€â”€ c_36
    â””â”€â”€ c_37
        â”œâ”€â”€ img_name.JPG
        â”œâ”€â”€ ...
        â””â”€â”€ img_name.JPG
</code></pre>
</div>

<p>Before feeding images into neural networks weâ€™ll normalize the images with mean and standard deviation of RGB channels computed from a random subset of ImageNet.</p>

<p>In the world of deep learning, dataset of 20,000 images is a relatively small dataset. 
Weâ€™ll therefore augment the data with randomly sized crops, color jittering and horizontal flips. 
Code to do these transformations is in <code class="highlighter-rouge">datasets/transforms.lua</code>. Most of it is borrowed from <a href="http://github.com/facebook/fb.resnet.torch">fb.resnet.torch</a> repo.</p>

<p>We will load the images in batches and do all these processing on the fly. This is done by writing a class named <code class="highlighter-rouge">DataGen</code> which does this. 
Essentially, code<span id="side1" class="margin-toggle sidenote-number"></span>
      <span class="sidenote">Understanding how iterators work in lua can be a little tricky. Read the following <a href="https://www.lua.org/pil/7.1.html"> documentation</a> for details.</span> can be summarized as :</p>

<p><code class="highlighter-rouge">datasets/plantvillage.lua</code>:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>require 'paths'
t = require 'datasets/transforms.lua'

function DataGen:__init(path)
    -- path is path of directory containing 'train' and 'val' folders
    self.rootPath = path
    self.trainImgPaths = self.findImages(paths.concat(self.rootPath, 'train'))
    self.valImgPaths = self.findImages(paths.concat(self.rootPath, 'val'))
    self.nbTrainExamples = #self.trainImgPaths
    self.nbValExamples = #self.valImgPaths 
end

-- Some utility functions
function DataGen.findImages(dir)
    -- Returns a table with all the image paths found in dir using 'find'
    ...
end

local function getClass(path)
    -- gets class from the name of the parent directory
    local className = paths.basename(paths.dirname(path))
    return tonumber(className:sub(3)) + 1
end

--- Iterator 
function DataGen:generator(pathsList, batchSize, preprocess) 
    -- pathsList is table with paths of images to be iterated over
    -- batchSize is number of images to be loaded in one iteration
    -- preprocess is function which will be applied to image after it's loaded

    -- Split all the paths into random batches
    local pathIndices = torch.randperm(#pathsList)
    local batches = pathIndices:split(batchSize)
    local i = 1
   
    return function ()
        if i &lt;= #batches then
            local currentBatch = batches[i]         

            local X = torch.Tensor(currentBatch:size(1), 3, 224, 224)
            local Y = torch.Tensor(currentBatch:size(1))

            for j = 1, currentBatch:size(1) do
                local currentPath = pathsList[currentBatch[j]]
                X[j] = preprocess(t.loadImage(currentPath))
                Y[j] = getClass(currentPath)
            end

            i = i + 1
            return X, Y
        end
   end
end

function DataGen:trainGenerator(batchSize)
    local trainPreprocess = t.Compose{
        t.RandomSizedCrop(224),
        t.ColorJitter({
            brightness = 0.4,
            contrast = 0.4,
            saturation = 0.4,
        }),
        t.Lighting(0.1, t.pca.eigval, t.pca.eigvec),
        t.ColorNormalize(t.meanstd),
        t.HorizontalFlip(0.5),}

   return self:generator(self.trainImgPaths, batchSize, trainPreprocess)
end


function DataGen:valGenerator(batchSize)
    local valPreprocess = t.Compose{
         t.Scale(256),
         t.ColorNormalize(t.meanstd),
         t.CenterCrop(224),}
   return self:generator(self.valImgPaths, batchSize, valPreprocess)
end

</code></pre>
</div>

<p>Complete code for this class with some error catching is at <code class="highlighter-rouge">datasets/plantvillage.lua</code>. 
We can now simply use a <code class="highlighter-rouge">DataGen</code> object to write a <code class="highlighter-rouge">for</code> loop to iterate over all the images:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>for input, target in dataGen:trainGenerator(batchSize) do
    -- code to train your model
end
</code></pre>
</div>

<p>Neat, isnâ€™t it?</p>

<h2 id="models">Models</h2>
<p>Now that weâ€™re done with loading and preprocessing the image files, letâ€™s start writing model descriptions.
These are the actual powerhouses that will be trained to classify. 
Itâ€™s quite easy to code up a model.</p>

<p>Letâ€™s code up alexnet as an example:</p>

<p><code class="highlighter-rouge">models/alexnet.lua</code>:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>require 'nn'

function createModel(opt)
    opt = opt or {}
    nbClasses = opt.nbClasses or 38
    nbChannels = opt.nbChannels or 3

    local features = nn.Sequential()
    
    features:add(nn.SpatialConvolution(nbChannels,64,11,11,4,4,2,2))    -- 224 -&gt; 55
    features:add(nn.SpatialMaxPooling(3,3,2,2))                   --  55 -&gt;  27
    features:add(nn.ReLU(true))
    features:add(nn.SpatialBatchNormalization(64))
    
    features:add(nn.SpatialConvolution(64,192,5,5,1,1,2,2))       --  27 -&gt; 27
    features:add(nn.SpatialMaxPooling(3,3,2,2))                   --  27 -&gt;  13
    features:add(nn.ReLU(true))
    features:add(nn.SpatialBatchNormalization(192))
    
    features:add(nn.SpatialConvolution(192,384,3,3,1,1,1,1))      --  13 -&gt;  13
    features:add(nn.ReLU(true))
    features:add(nn.SpatialBatchNormalization(384))
    
    features:add(nn.SpatialConvolution(384,256,3,3,1,1,1,1))      --  13 -&gt;  13
    features:add(nn.ReLU(true))
    features:add(nn.SpatialBatchNormalization(256))
    
    features:add(nn.SpatialConvolution(256,256,3,3,1,1,1,1))      --  13 -&gt;  13
    features:add(nn.SpatialMaxPooling(3,3,2,2))                   --  13 -&gt; 6
    features:add(nn.ReLU(true))
    features:add(nn.SpatialBatchNormalization(256))

    local classifier = nn.Sequential()
    classifier:add(nn.View(256*6*6))
    
    classifier:add(nn.Dropout(0.5))
    classifier:add(nn.Linear(256*6*6, 4096))
    classifier:add(nn.ReLU(true))
    
    classifier:add(nn.Dropout(0.5))
    classifier:add(nn.Linear(4096, 4096))
    classifier:add(nn.ReLU(true))
    
    classifier:add(nn.Linear(4096, nbClasses))

    model:add(features):add(classifier)

    return model
end
</code></pre>
</div>

<p>As you can see, model is just a stack of convolutional layers, max pooling and fully connected layers.</p>

<p>We will use <code class="highlighter-rouge">nn.CrossEntropyCriterion</code> as our criterion.</p>

<h2 id="training">Training</h2>

<p>In torch, <code class="highlighter-rouge">net:forward(input)</code> computes the <code class="highlighter-rouge">output</code> of neural network. 
This is the forward pass of the backpropagation algorithm while <code class="highlighter-rouge">net:backward(input,gradOutput)</code> is the backward pass of the backpropagation. 
Forward and backward passes for <code class="highlighter-rouge">criterion</code> are also very similar.</p>

<p>Weâ€™ll use <a href="https://github.com/torch/optim/blob/master/doc/index.md#optim.adam">adam</a> optimizer from <code class="highlighter-rouge">optim</code> package to make the updates to the network. A minimal training script will look like:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>require 'nn'
require 'datasets/plantvillage.lua'
require 'models/alexnet.lua'

net = createModel()
criterion = nn.CrossEntropyCriterion()
dataGen = DataGen('path/to/folder/with/train-val-directories/')

-- adam initial learning rate and momentum parameters
optimState = { 
        learningRate = 0.01,
        beta1 = 0.9,
    }

-- Parameters for network that need to be optimized
params, gradParams = net:getParameters()

local function feval()
    return criterion.output, gradParams
end

function train()
    for input, target in dataGen:trainGenerator(batchSize) do
        -- Forward pass
        output = net:forward(input)
        criterion:forward(output, target)

        -- Backward pass
        net:zeroGradParameters()
        critGrad = criterion:backward(output, target)
        net:backward(input, critGrad)
        
        -- Make updates using adam
        optim.adam(feval, params, optimState)
    end
end

for i = 1, nbEpochs do
    train()
end
</code></pre>
</div>

<p>If you look at the code in the repo, youâ€™ll find that I have divided training into <code class="highlighter-rouge">main.py</code> and <code class="highlighter-rouge">train.py</code> scripts.
In <code class="highlighter-rouge">main.lua</code>, we manage the configuration of the neural network and criterion.
In <code class="highlighter-rouge">train.lua</code>, I wrote a <code class="highlighter-rouge">Trainer</code> class with <code class="highlighter-rouge">Trainer:train()</code> and <code class="highlighter-rouge">Trainer:validate()</code> methods very similar to <code class="highlighter-rouge">train()</code> function above except with some logging.</p>

  </section>

    <footer class="site-footer">
   <section class="copyright">All content copyright <a href="mailto:sasankchilamkurthy@gmail.com">Sasank Chilamkurthy</a> &copy; 2016 &bull; All rights reserved.</section>
</footer>


  </div>
</article>

  </body>
</html>
